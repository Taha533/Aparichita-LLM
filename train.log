Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.13it/s]
Trainable parameter: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.32.self_attn.q_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.32.self_attn.q_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.32.self_attn.k_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.32.self_attn.k_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.32.self_attn.v_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.32.self_attn.v_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.33.self_attn.q_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.33.self_attn.q_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.33.self_attn.k_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.33.self_attn.k_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.33.self_attn.v_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.33.self_attn.v_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.34.self_attn.q_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.34.self_attn.q_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.34.self_attn.k_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.34.self_attn.k_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.34.self_attn.v_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.34.self_attn.v_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.35.self_attn.q_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.35.self_attn.q_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.35.self_attn.k_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.35.self_attn.k_proj.lora_B.default.weight
Trainable parameter: base_model.model.model.layers.35.self_attn.v_proj.lora_A.default.weight
Trainable parameter: base_model.model.model.layers.35.self_attn.v_proj.lora_B.default.weight
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
{'loss': 2.8269, 'grad_norm': 17.46535873413086, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.29}                                                                
{'loss': 2.3337, 'grad_norm': 8.892752647399902, 'learning_rate': 1.8e-05, 'epoch': 0.58}                                                                              
{'loss': 2.0491, 'grad_norm': 5.243870735168457, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.87}                                                               
{'loss': 1.6826, 'grad_norm': 5.423349857330322, 'learning_rate': 3.8e-05, 'epoch': 1.14}                                                                              
{'loss': 1.5519, 'grad_norm': 5.9312567710876465, 'learning_rate': 4.8e-05, 'epoch': 1.43}                                                                             
{'loss': 1.4069, 'grad_norm': 8.401878356933594, 'learning_rate': 4.9884057971014496e-05, 'epoch': 1.72}                                                               
{'loss': 1.3093, 'grad_norm': 9.677375793457031, 'learning_rate': 4.973913043478261e-05, 'epoch': 2.0}                                                                 
{'loss': 1.1417, 'grad_norm': 6.5521674156188965, 'learning_rate': 4.959420289855073e-05, 'epoch': 2.29}                                                               
{'loss': 1.1438, 'grad_norm': 7.367995262145996, 'learning_rate': 4.9449275362318844e-05, 'epoch': 2.58}                                                               
{'loss': 1.1484, 'grad_norm': 4.927903175354004, 'learning_rate': 4.930434782608696e-05, 'epoch': 2.87}                                                                
{'loss': 0.8952, 'grad_norm': 4.620516300201416, 'learning_rate': 4.915942028985507e-05, 'epoch': 3.14}                                                                
{'loss': 1.0245, 'grad_norm': 6.416475296020508, 'learning_rate': 4.901449275362319e-05, 'epoch': 3.43}                                                                
{'loss': 0.9335, 'grad_norm': 6.03856897354126, 'learning_rate': 4.8869565217391305e-05, 'epoch': 3.72}                                                                
{'loss': 0.8475, 'grad_norm': 6.794556617736816, 'learning_rate': 4.8724637681159426e-05, 'epoch': 4.0}                                                                
{'loss': 0.8024, 'grad_norm': 6.766456604003906, 'learning_rate': 4.857971014492754e-05, 'epoch': 4.29}                                                                
{'loss': 0.794, 'grad_norm': 4.705687522888184, 'learning_rate': 4.843478260869565e-05, 'epoch': 4.58}                                                                 
{'loss': 0.7843, 'grad_norm': 6.505565166473389, 'learning_rate': 4.828985507246377e-05, 'epoch': 4.87}                                                                
{'loss': 0.8015, 'grad_norm': 6.008866310119629, 'learning_rate': 4.814492753623189e-05, 'epoch': 5.14}                                                                
{'loss': 0.7333, 'grad_norm': 7.639520168304443, 'learning_rate': 4.8e-05, 'epoch': 5.43}                                                                              
{'loss': 0.6279, 'grad_norm': 4.788392543792725, 'learning_rate': 4.785507246376812e-05, 'epoch': 5.72}                                                                
{'loss': 0.6925, 'grad_norm': 10.487103462219238, 'learning_rate': 4.7710144927536235e-05, 'epoch': 6.0}                                                               
{'loss': 0.5688, 'grad_norm': 6.668778419494629, 'learning_rate': 4.756521739130435e-05, 'epoch': 6.29}                                                                
{'loss': 0.6373, 'grad_norm': 8.811616897583008, 'learning_rate': 4.742028985507246e-05, 'epoch': 6.58}                                                                
{'loss': 0.5994, 'grad_norm': 8.35830020904541, 'learning_rate': 4.727536231884058e-05, 'epoch': 6.87}                                                                 
{'loss': 0.5495, 'grad_norm': 6.410679817199707, 'learning_rate': 4.71304347826087e-05, 'epoch': 7.14}                                                                 
{'loss': 0.5513, 'grad_norm': 13.451018333435059, 'learning_rate': 4.698550724637682e-05, 'epoch': 7.43}                                                               
{'loss': 0.4923, 'grad_norm': 7.646419048309326, 'learning_rate': 4.6840579710144924e-05, 'epoch': 7.72}                                                               
{'loss': 0.5235, 'grad_norm': 13.399269104003906, 'learning_rate': 4.6695652173913045e-05, 'epoch': 8.0}                                                               
{'loss': 0.3707, 'grad_norm': 8.506961822509766, 'learning_rate': 4.655072463768116e-05, 'epoch': 8.29}                                                                
{'loss': 0.4308, 'grad_norm': 10.941267013549805, 'learning_rate': 4.640579710144928e-05, 'epoch': 8.58}                                                               
{'loss': 0.4331, 'grad_norm': 9.774643898010254, 'learning_rate': 4.62608695652174e-05, 'epoch': 8.87}                                                                 
{'loss': 0.439, 'grad_norm': 11.945658683776855, 'learning_rate': 4.6115942028985506e-05, 'epoch': 9.14}                                                               
{'loss': 0.3659, 'grad_norm': 7.539191246032715, 'learning_rate': 4.597101449275363e-05, 'epoch': 9.43}                                                                
{'loss': 0.3362, 'grad_norm': 10.368696212768555, 'learning_rate': 4.582608695652174e-05, 'epoch': 9.72}                                                               
{'loss': 0.3545, 'grad_norm': 11.682770729064941, 'learning_rate': 4.568115942028986e-05, 'epoch': 10.0}                                                               
{'loss': 0.3466, 'grad_norm': 14.121424674987793, 'learning_rate': 4.555072463768116e-05, 'epoch': 10.29}                                                              
{'loss': 0.3, 'grad_norm': 7.672552108764648, 'learning_rate': 4.5405797101449275e-05, 'epoch': 10.58}                                                                 
{'loss': 0.2657, 'grad_norm': 5.55119514465332, 'learning_rate': 4.5260869565217395e-05, 'epoch': 10.87}                                                               
{'loss': 0.2799, 'grad_norm': 8.291803359985352, 'learning_rate': 4.511594202898551e-05, 'epoch': 11.14}                                                               
{'loss': 0.2741, 'grad_norm': 10.695225715637207, 'learning_rate': 4.497101449275363e-05, 'epoch': 11.43}                                                              
{'loss': 0.2571, 'grad_norm': 8.191497802734375, 'learning_rate': 4.482608695652174e-05, 'epoch': 11.72}                                                               
{'loss': 0.2967, 'grad_norm': 21.549840927124023, 'learning_rate': 4.468115942028986e-05, 'epoch': 12.0}                                                               
{'loss': 0.241, 'grad_norm': 7.303228378295898, 'learning_rate': 4.453623188405797e-05, 'epoch': 12.29}                                                                
{'loss': 0.2335, 'grad_norm': 10.37255573272705, 'learning_rate': 4.439130434782609e-05, 'epoch': 12.58}                                                               
{'loss': 0.2107, 'grad_norm': 7.248197555541992, 'learning_rate': 4.4246376811594205e-05, 'epoch': 12.87}                                                              
{'loss': 0.2398, 'grad_norm': 4.400554656982422, 'learning_rate': 4.4101449275362325e-05, 'epoch': 13.14}                                                              
{'loss': 0.1781, 'grad_norm': 5.023792743682861, 'learning_rate': 4.395652173913043e-05, 'epoch': 13.43}                                                               
{'loss': 0.2282, 'grad_norm': 11.321115493774414, 'learning_rate': 4.381159420289855e-05, 'epoch': 13.72}                                                              
{'loss': 0.2197, 'grad_norm': 6.015691757202148, 'learning_rate': 4.3666666666666666e-05, 'epoch': 14.0}                                                               
{'loss': 0.1792, 'grad_norm': 6.191547393798828, 'learning_rate': 4.352173913043479e-05, 'epoch': 14.29}                                                               
{'loss': 0.1988, 'grad_norm': 5.0475850105285645, 'learning_rate': 4.33768115942029e-05, 'epoch': 14.58}                                                               
{'loss': 0.1833, 'grad_norm': 7.628658771514893, 'learning_rate': 4.3231884057971014e-05, 'epoch': 14.87}                                                              
{'loss': 0.1923, 'grad_norm': 8.048065185546875, 'learning_rate': 4.308695652173913e-05, 'epoch': 15.14}                                                               
{'loss': 0.1808, 'grad_norm': 11.31595230102539, 'learning_rate': 4.294202898550725e-05, 'epoch': 15.43}                                                               
{'loss': 0.1586, 'grad_norm': 4.482953071594238, 'learning_rate': 4.279710144927536e-05, 'epoch': 15.72}                                                               
{'loss': 0.1935, 'grad_norm': 8.091588973999023, 'learning_rate': 4.265217391304348e-05, 'epoch': 16.0}                                                                
{'loss': 0.1537, 'grad_norm': 4.452296733856201, 'learning_rate': 4.2507246376811596e-05, 'epoch': 16.29}                                                              
{'loss': 0.1751, 'grad_norm': 6.3530378341674805, 'learning_rate': 4.236231884057971e-05, 'epoch': 16.58}                                                              
{'loss': 0.1618, 'grad_norm': 2.6919450759887695, 'learning_rate': 4.221739130434783e-05, 'epoch': 16.87}                                                              
{'loss': 0.1549, 'grad_norm': 2.558511257171631, 'learning_rate': 4.2072463768115944e-05, 'epoch': 17.14}                                                              
{'loss': 0.1544, 'grad_norm': 5.057734966278076, 'learning_rate': 4.1927536231884065e-05, 'epoch': 17.43}                                                              
{'loss': 0.1813, 'grad_norm': 9.751606941223145, 'learning_rate': 4.178260869565218e-05, 'epoch': 17.72}                                                               
{'loss': 0.1884, 'grad_norm': 20.763402938842773, 'learning_rate': 4.163768115942029e-05, 'epoch': 18.0}                                                               
{'loss': 0.1272, 'grad_norm': 7.236425876617432, 'learning_rate': 4.1492753623188406e-05, 'epoch': 18.29}                                                              
{'loss': 0.1838, 'grad_norm': 5.287649154663086, 'learning_rate': 4.1347826086956526e-05, 'epoch': 18.58}                                                              
{'loss': 0.1566, 'grad_norm': 8.407654762268066, 'learning_rate': 4.120289855072464e-05, 'epoch': 18.87}                                                               
{'loss': 0.1516, 'grad_norm': 5.137848377227783, 'learning_rate': 4.105797101449276e-05, 'epoch': 19.14}                                                               
{'loss': 0.1412, 'grad_norm': 2.5410268306732178, 'learning_rate': 4.091304347826087e-05, 'epoch': 19.43}                                                              
{'loss': 0.1644, 'grad_norm': 2.917565107345581, 'learning_rate': 4.076811594202899e-05, 'epoch': 19.72}                                                               
{'loss': 0.1583, 'grad_norm': 4.5547404289245605, 'learning_rate': 4.06231884057971e-05, 'epoch': 20.0}                                                                
{'loss': 0.1406, 'grad_norm': 2.8614401817321777, 'learning_rate': 4.047826086956522e-05, 'epoch': 20.29}                                                              
{'loss': 0.1365, 'grad_norm': 3.1675827503204346, 'learning_rate': 4.0333333333333336e-05, 'epoch': 20.58}                                                             
{'loss': 0.1607, 'grad_norm': 2.7175374031066895, 'learning_rate': 4.018840579710145e-05, 'epoch': 20.87}                                                              
{'loss': 0.1479, 'grad_norm': 2.106858968734741, 'learning_rate': 4.004347826086956e-05, 'epoch': 21.14}                                                               
{'loss': 0.1483, 'grad_norm': 5.593585968017578, 'learning_rate': 3.9898550724637684e-05, 'epoch': 21.43}                                                              
{'loss': 0.147, 'grad_norm': 3.7367331981658936, 'learning_rate': 3.97536231884058e-05, 'epoch': 21.72}                                                                
{'loss': 0.1594, 'grad_norm': 6.742217540740967, 'learning_rate': 3.960869565217392e-05, 'epoch': 22.0}                                                                
{'loss': 0.1425, 'grad_norm': 3.6347122192382812, 'learning_rate': 3.946376811594203e-05, 'epoch': 22.29}                                                              
{'loss': 0.1434, 'grad_norm': 3.9271035194396973, 'learning_rate': 3.9318840579710145e-05, 'epoch': 22.58}                                                             
{'loss': 0.143, 'grad_norm': 4.145927906036377, 'learning_rate': 3.917391304347826e-05, 'epoch': 22.87}                                                                
{'loss': 0.1399, 'grad_norm': 2.9592223167419434, 'learning_rate': 3.902898550724638e-05, 'epoch': 23.14}                                                              
{'loss': 0.137, 'grad_norm': 4.528726577758789, 'learning_rate': 3.888405797101449e-05, 'epoch': 23.43}                                                                
{'loss': 0.1518, 'grad_norm': 3.9754605293273926, 'learning_rate': 3.8739130434782613e-05, 'epoch': 23.72}                                                             
{'loss': 0.1563, 'grad_norm': 4.678042411804199, 'learning_rate': 3.859420289855073e-05, 'epoch': 24.0}                                                                
{'loss': 0.1305, 'grad_norm': 5.422916412353516, 'learning_rate': 3.844927536231884e-05, 'epoch': 24.29}                                                               
{'loss': 0.1363, 'grad_norm': 2.3780407905578613, 'learning_rate': 3.8304347826086955e-05, 'epoch': 24.58}                                                             
{'loss': 0.1528, 'grad_norm': 4.423656940460205, 'learning_rate': 3.8159420289855075e-05, 'epoch': 24.87}                                                              
{'loss': 0.1396, 'grad_norm': 2.347992181777954, 'learning_rate': 3.801449275362319e-05, 'epoch': 25.14}                                                               
{'loss': 0.1382, 'grad_norm': 3.1558430194854736, 'learning_rate': 3.786956521739131e-05, 'epoch': 25.43}                                                              
{'loss': 0.1251, 'grad_norm': 2.1752312183380127, 'learning_rate': 3.772463768115942e-05, 'epoch': 25.72}                                                              
{'loss': 0.1489, 'grad_norm': 5.770974159240723, 'learning_rate': 3.7579710144927537e-05, 'epoch': 26.0}                                                               
{'loss': 0.1216, 'grad_norm': 2.8140804767608643, 'learning_rate': 3.743478260869566e-05, 'epoch': 26.29}                                                              
{'loss': 0.1285, 'grad_norm': 2.3842058181762695, 'learning_rate': 3.728985507246377e-05, 'epoch': 26.58}                                                              
{'loss': 0.1554, 'grad_norm': 5.449615478515625, 'learning_rate': 3.714492753623189e-05, 'epoch': 26.87}                                                               
{'loss': 0.1305, 'grad_norm': 2.287532091140747, 'learning_rate': 3.7e-05, 'epoch': 27.14}                                                                             
{'loss': 0.1406, 'grad_norm': 1.9856295585632324, 'learning_rate': 3.685507246376812e-05, 'epoch': 27.43}                                                              
{'loss': 0.1257, 'grad_norm': 4.901989459991455, 'learning_rate': 3.671014492753623e-05, 'epoch': 27.72}                                                               
{'loss': 0.1319, 'grad_norm': 3.2423999309539795, 'learning_rate': 3.656521739130435e-05, 'epoch': 28.0}                                                               
{'loss': 0.1257, 'grad_norm': 2.432410955429077, 'learning_rate': 3.6420289855072466e-05, 'epoch': 28.29}                                                              
{'loss': 0.128, 'grad_norm': 3.138967275619507, 'learning_rate': 3.627536231884058e-05, 'epoch': 28.58}                                                                
{'loss': 0.1528, 'grad_norm': 2.507420539855957, 'learning_rate': 3.6130434782608694e-05, 'epoch': 28.87}                                                              
{'loss': 0.113, 'grad_norm': 1.904950737953186, 'learning_rate': 3.5985507246376814e-05, 'epoch': 29.14}                                                               
{'loss': 0.1276, 'grad_norm': 3.8497023582458496, 'learning_rate': 3.584057971014493e-05, 'epoch': 29.43}                                                              
{'loss': 0.1294, 'grad_norm': 2.3131768703460693, 'learning_rate': 3.569565217391305e-05, 'epoch': 29.72}                                                              
{'loss': 0.1439, 'grad_norm': 7.860903263092041, 'learning_rate': 3.555072463768116e-05, 'epoch': 30.0}                                                                
{'loss': 0.1175, 'grad_norm': 3.177438974380493, 'learning_rate': 3.5405797101449276e-05, 'epoch': 30.29}                                                              
{'loss': 0.131, 'grad_norm': 1.8886462450027466, 'learning_rate': 3.526086956521739e-05, 'epoch': 30.58}                                                               
{'loss': 0.1568, 'grad_norm': 2.241849422454834, 'learning_rate': 3.511594202898551e-05, 'epoch': 30.87}                                                               
{'loss': 0.123, 'grad_norm': 2.324989080429077, 'learning_rate': 3.4971014492753624e-05, 'epoch': 31.14}                                                               
{'loss': 0.12, 'grad_norm': 2.824054479598999, 'learning_rate': 3.4826086956521744e-05, 'epoch': 31.43}                                                                
{'loss': 0.1253, 'grad_norm': 1.9799035787582397, 'learning_rate': 3.468115942028986e-05, 'epoch': 31.72}                                                              
{'loss': 0.1507, 'grad_norm': 8.106583595275879, 'learning_rate': 3.453623188405797e-05, 'epoch': 32.0}                                                                
{'loss': 0.1318, 'grad_norm': 3.0872576236724854, 'learning_rate': 3.4391304347826085e-05, 'epoch': 32.29}                                                             
{'loss': 0.1246, 'grad_norm': 2.0780158042907715, 'learning_rate': 3.4246376811594206e-05, 'epoch': 32.58}                                                             
{'loss': 0.1266, 'grad_norm': 2.632066488265991, 'learning_rate': 3.410144927536232e-05, 'epoch': 32.87}                                                               
{'loss': 0.1195, 'grad_norm': 1.8569413423538208, 'learning_rate': 3.395652173913044e-05, 'epoch': 33.14}                                                              
{'loss': 0.1231, 'grad_norm': 2.716326951980591, 'learning_rate': 3.381159420289855e-05, 'epoch': 33.43}                                                               
{'loss': 0.1202, 'grad_norm': 2.6588540077209473, 'learning_rate': 3.366666666666667e-05, 'epoch': 33.72}                                                              
{'loss': 0.1407, 'grad_norm': 3.069265604019165, 'learning_rate': 3.352173913043478e-05, 'epoch': 34.0}                                                                
{'loss': 0.1103, 'grad_norm': 2.1874396800994873, 'learning_rate': 3.33768115942029e-05, 'epoch': 34.29}                                                               
{'loss': 0.118, 'grad_norm': 1.6201226711273193, 'learning_rate': 3.3231884057971015e-05, 'epoch': 34.58}                                                              
{'loss': 0.1376, 'grad_norm': 1.2507214546203613, 'learning_rate': 3.308695652173913e-05, 'epoch': 34.87}                                                              
{'loss': 0.1283, 'grad_norm': 1.2221853733062744, 'learning_rate': 3.294202898550725e-05, 'epoch': 35.14}                                                              
{'loss': 0.1164, 'grad_norm': 1.636952519416809, 'learning_rate': 3.279710144927536e-05, 'epoch': 35.43}                                                               
{'loss': 0.1277, 'grad_norm': 2.059809684753418, 'learning_rate': 3.2652173913043484e-05, 'epoch': 35.72}                                                              
{'loss': 0.1497, 'grad_norm': 7.0417680740356445, 'learning_rate': 3.25072463768116e-05, 'epoch': 36.0}                                                                
{'loss': 0.108, 'grad_norm': 2.753427028656006, 'learning_rate': 3.236231884057971e-05, 'epoch': 36.29}                                                                
{'loss': 0.1318, 'grad_norm': 2.688035249710083, 'learning_rate': 3.2217391304347825e-05, 'epoch': 36.58}                                                              
{'loss': 0.1421, 'grad_norm': 1.660354733467102, 'learning_rate': 3.2072463768115945e-05, 'epoch': 36.87}                                                              
{'loss': 0.1134, 'grad_norm': 1.738939881324768, 'learning_rate': 3.192753623188406e-05, 'epoch': 37.14}                                                               
{'loss': 0.1164, 'grad_norm': 1.852542757987976, 'learning_rate': 3.178260869565218e-05, 'epoch': 37.43}                                                               
{'loss': 0.1248, 'grad_norm': 6.5841898918151855, 'learning_rate': 3.163768115942029e-05, 'epoch': 37.72}                                                              
{'loss': 0.142, 'grad_norm': 3.815471887588501, 'learning_rate': 3.149275362318841e-05, 'epoch': 38.0}                                                                 
{'loss': 0.1214, 'grad_norm': 7.072790622711182, 'learning_rate': 3.134782608695652e-05, 'epoch': 38.29}                                                               
{'loss': 0.1209, 'grad_norm': 3.3196959495544434, 'learning_rate': 3.120289855072464e-05, 'epoch': 38.58}                                                              
{'loss': 0.1194, 'grad_norm': 1.7111057043075562, 'learning_rate': 3.1057971014492755e-05, 'epoch': 38.87}                                                             
{'loss': 0.1246, 'grad_norm': 1.8165819644927979, 'learning_rate': 3.0913043478260875e-05, 'epoch': 39.14}                                                             
{'loss': 0.1188, 'grad_norm': 2.6648290157318115, 'learning_rate': 3.076811594202899e-05, 'epoch': 39.43}                                                              
{'loss': 0.1203, 'grad_norm': 1.7526556253433228, 'learning_rate': 3.06231884057971e-05, 'epoch': 39.72}                                                               
{'loss': 0.1212, 'grad_norm': 2.175570487976074, 'learning_rate': 3.0478260869565216e-05, 'epoch': 40.0}                                                               
{'loss': 0.1111, 'grad_norm': 1.6694211959838867, 'learning_rate': 3.0333333333333337e-05, 'epoch': 40.29}                                                             
{'loss': 0.1252, 'grad_norm': 2.2502901554107666, 'learning_rate': 3.018840579710145e-05, 'epoch': 40.58}                                                              
{'loss': 0.1236, 'grad_norm': 1.51032555103302, 'learning_rate': 3.0043478260869567e-05, 'epoch': 40.87}                                                               
{'loss': 0.1185, 'grad_norm': 1.6046229600906372, 'learning_rate': 2.989855072463768e-05, 'epoch': 41.14}                                                              
{'loss': 0.1218, 'grad_norm': 2.4201667308807373, 'learning_rate': 2.9753623188405798e-05, 'epoch': 41.43}                                                             
{'loss': 0.1257, 'grad_norm': 1.3534457683563232, 'learning_rate': 2.9608695652173912e-05, 'epoch': 41.72}                                                             
{'loss': 0.1123, 'grad_norm': 2.7659759521484375, 'learning_rate': 2.9463768115942032e-05, 'epoch': 42.0}                                                              
{'loss': 0.1026, 'grad_norm': 1.505822777748108, 'learning_rate': 2.9318840579710143e-05, 'epoch': 42.29}                                                              
{'loss': 0.1261, 'grad_norm': 2.5246975421905518, 'learning_rate': 2.9173913043478263e-05, 'epoch': 42.58}                                                             
{'loss': 0.1326, 'grad_norm': 1.8822435140609741, 'learning_rate': 2.9028985507246377e-05, 'epoch': 42.87}                                                             
{'loss': 0.1193, 'grad_norm': 1.4238451719284058, 'learning_rate': 2.8884057971014494e-05, 'epoch': 43.14}                                                             
{'loss': 0.1168, 'grad_norm': 2.5300846099853516, 'learning_rate': 2.8739130434782608e-05, 'epoch': 43.43}                                                             
{'loss': 0.1164, 'grad_norm': 1.4077191352844238, 'learning_rate': 2.8594202898550725e-05, 'epoch': 43.72}                                                             
{'loss': 0.1204, 'grad_norm': 1.8005242347717285, 'learning_rate': 2.8449275362318845e-05, 'epoch': 44.0}                                                              
{'loss': 0.1072, 'grad_norm': 2.6808552742004395, 'learning_rate': 2.830434782608696e-05, 'epoch': 44.29}                                                              
{'loss': 0.1254, 'grad_norm': 3.189077138900757, 'learning_rate': 2.8159420289855076e-05, 'epoch': 44.58}                                                              
{'loss': 0.1216, 'grad_norm': 1.6539015769958496, 'learning_rate': 2.801449275362319e-05, 'epoch': 44.87}                                                              
{'loss': 0.1194, 'grad_norm': 2.4023356437683105, 'learning_rate': 2.7869565217391307e-05, 'epoch': 45.14}                                                             
{'loss': 0.1171, 'grad_norm': 1.98179030418396, 'learning_rate': 2.772463768115942e-05, 'epoch': 45.43}                                                                
{'loss': 0.1257, 'grad_norm': 1.8908085823059082, 'learning_rate': 2.757971014492754e-05, 'epoch': 45.72}                                                              
{'loss': 0.1199, 'grad_norm': 1.8762646913528442, 'learning_rate': 2.743478260869565e-05, 'epoch': 46.0}                                                               
{'loss': 0.1132, 'grad_norm': 2.968916177749634, 'learning_rate': 2.728985507246377e-05, 'epoch': 46.29}                                                               
{'loss': 0.1039, 'grad_norm': 2.0008959770202637, 'learning_rate': 2.7144927536231885e-05, 'epoch': 46.58}                                                             
{'loss': 0.1301, 'grad_norm': 1.4824321269989014, 'learning_rate': 2.7000000000000002e-05, 'epoch': 46.87}                                                             
{'loss': 0.1251, 'grad_norm': 1.601187825202942, 'learning_rate': 2.6855072463768116e-05, 'epoch': 47.14}                                                              
{'loss': 0.1121, 'grad_norm': 2.000805139541626, 'learning_rate': 2.6710144927536233e-05, 'epoch': 47.43}                                                              
{'loss': 0.122, 'grad_norm': 1.4170631170272827, 'learning_rate': 2.6565217391304347e-05, 'epoch': 47.72}                                                              
{'loss': 0.1173, 'grad_norm': 2.056196689605713, 'learning_rate': 2.6420289855072467e-05, 'epoch': 48.0}                                                               
{'loss': 0.1073, 'grad_norm': 1.3398149013519287, 'learning_rate': 2.627536231884058e-05, 'epoch': 48.29}                                                              
{'loss': 0.1086, 'grad_norm': 1.8135654926300049, 'learning_rate': 2.6130434782608698e-05, 'epoch': 48.58}                                                             
{'loss': 0.1302, 'grad_norm': 1.7188043594360352, 'learning_rate': 2.5985507246376812e-05, 'epoch': 48.87}                                                             
{'loss': 0.1137, 'grad_norm': 1.293264389038086, 'learning_rate': 2.584057971014493e-05, 'epoch': 49.14}                                                               
{'loss': 0.1012, 'grad_norm': 1.7791321277618408, 'learning_rate': 2.5695652173913043e-05, 'epoch': 49.43}                                                             
{'loss': 0.13, 'grad_norm': 1.971124529838562, 'learning_rate': 2.5550724637681163e-05, 'epoch': 49.72}                                                                
{'loss': 0.1193, 'grad_norm': 2.698573112487793, 'learning_rate': 2.5405797101449273e-05, 'epoch': 50.0}                                                               
{'loss': 0.1009, 'grad_norm': 1.8917739391326904, 'learning_rate': 2.5260869565217394e-05, 'epoch': 50.29}                                                             
{'loss': 0.115, 'grad_norm': 1.51505446434021, 'learning_rate': 2.5115942028985508e-05, 'epoch': 50.58}                                                                
{'loss': 0.1285, 'grad_norm': 1.9949942827224731, 'learning_rate': 2.4971014492753625e-05, 'epoch': 50.87}                                                             
{'loss': 0.1139, 'grad_norm': 1.2970792055130005, 'learning_rate': 2.4826086956521742e-05, 'epoch': 51.14}                                                             
{'loss': 0.096, 'grad_norm': 1.6486772298812866, 'learning_rate': 2.4681159420289855e-05, 'epoch': 51.43}                                                              
{'loss': 0.1138, 'grad_norm': 1.8044511079788208, 'learning_rate': 2.4536231884057972e-05, 'epoch': 51.72}                                                             
{'loss': 0.1486, 'grad_norm': 4.45676326751709, 'learning_rate': 2.439130434782609e-05, 'epoch': 52.0}                                                                 
{'loss': 0.1023, 'grad_norm': 1.1927988529205322, 'learning_rate': 2.4246376811594203e-05, 'epoch': 52.29}                                                             
{'loss': 0.1172, 'grad_norm': 1.8048226833343506, 'learning_rate': 2.410144927536232e-05, 'epoch': 52.58}                                                              
{'loss': 0.1115, 'grad_norm': 1.7772293090820312, 'learning_rate': 2.3956521739130437e-05, 'epoch': 52.87}                                                             
{'loss': 0.1215, 'grad_norm': 1.3926249742507935, 'learning_rate': 2.381159420289855e-05, 'epoch': 53.14}                                                              
{'loss': 0.1156, 'grad_norm': 1.8172218799591064, 'learning_rate': 2.3666666666666668e-05, 'epoch': 53.43}                                                             
{'loss': 0.1232, 'grad_norm': 2.111555337905884, 'learning_rate': 2.3521739130434782e-05, 'epoch': 53.72}                                                              
{'loss': 0.1063, 'grad_norm': 1.8017001152038574, 'learning_rate': 2.33768115942029e-05, 'epoch': 54.0}                                                                
{'loss': 0.1, 'grad_norm': 1.6075875759124756, 'learning_rate': 2.3231884057971016e-05, 'epoch': 54.29}                                                                
{'loss': 0.1189, 'grad_norm': 1.5272058248519897, 'learning_rate': 2.308695652173913e-05, 'epoch': 54.58}                                                              
{'loss': 0.13, 'grad_norm': 1.519768476486206, 'learning_rate': 2.2942028985507247e-05, 'epoch': 54.87}                                                                
{'loss': 0.1052, 'grad_norm': 1.756453514099121, 'learning_rate': 2.2797101449275364e-05, 'epoch': 55.14}                                                              
{'loss': 0.1127, 'grad_norm': 3.3850653171539307, 'learning_rate': 2.2652173913043478e-05, 'epoch': 55.43}                                                             
{'loss': 0.1249, 'grad_norm': 2.153843402862549, 'learning_rate': 2.2507246376811595e-05, 'epoch': 55.72}                                                              
{'loss': 0.1138, 'grad_norm': 2.7261931896209717, 'learning_rate': 2.2362318840579712e-05, 'epoch': 56.0}                                                              
{'loss': 0.1037, 'grad_norm': 1.4703621864318848, 'learning_rate': 2.2217391304347825e-05, 'epoch': 56.29}                                                             
{'loss': 0.1106, 'grad_norm': 1.7102333307266235, 'learning_rate': 2.2072463768115943e-05, 'epoch': 56.58}                                                             
{'loss': 0.1191, 'grad_norm': 1.3500670194625854, 'learning_rate': 2.1927536231884056e-05, 'epoch': 56.87}                                                             
{'loss': 0.1157, 'grad_norm': 1.6130260229110718, 'learning_rate': 2.1782608695652177e-05, 'epoch': 57.14}                                                             
{'loss': 0.1202, 'grad_norm': 1.7963840961456299, 'learning_rate': 2.1637681159420294e-05, 'epoch': 57.43}                                                             
{'loss': 0.1015, 'grad_norm': 1.4023317098617554, 'learning_rate': 2.1492753623188408e-05, 'epoch': 57.72}                                                             
{'loss': 0.1204, 'grad_norm': 2.1002447605133057, 'learning_rate': 2.1347826086956525e-05, 'epoch': 58.0}                                                              
{'loss': 0.1227, 'grad_norm': 1.20038902759552, 'learning_rate': 2.120289855072464e-05, 'epoch': 58.29}                                                                
{'loss': 0.1055, 'grad_norm': 1.342894434928894, 'learning_rate': 2.1057971014492755e-05, 'epoch': 58.58}                                                              
{'loss': 0.119, 'grad_norm': 1.663158655166626, 'learning_rate': 2.0913043478260872e-05, 'epoch': 58.87}                                                               
{'loss': 0.1031, 'grad_norm': 0.9501577615737915, 'learning_rate': 2.0768115942028986e-05, 'epoch': 59.14}                                                             
{'loss': 0.1169, 'grad_norm': 1.2082902193069458, 'learning_rate': 2.0623188405797103e-05, 'epoch': 59.43}                                                             
{'loss': 0.1134, 'grad_norm': 1.4500707387924194, 'learning_rate': 2.047826086956522e-05, 'epoch': 59.72}                                                              
{'loss': 0.1186, 'grad_norm': 3.439795732498169, 'learning_rate': 2.0333333333333334e-05, 'epoch': 60.0}                                                               
{'loss': 0.1114, 'grad_norm': 1.0879836082458496, 'learning_rate': 2.018840579710145e-05, 'epoch': 60.29}                                                              
{'loss': 0.1054, 'grad_norm': 1.5317180156707764, 'learning_rate': 2.0043478260869565e-05, 'epoch': 60.58}                                                             
{'loss': 0.1152, 'grad_norm': 1.5109087228775024, 'learning_rate': 1.9898550724637682e-05, 'epoch': 60.87}                                                             
{'loss': 0.104, 'grad_norm': 1.3247761726379395, 'learning_rate': 1.97536231884058e-05, 'epoch': 61.14}                                                                
{'loss': 0.113, 'grad_norm': 1.9378626346588135, 'learning_rate': 1.9608695652173913e-05, 'epoch': 61.43}                                                              
{'loss': 0.1188, 'grad_norm': 1.8261878490447998, 'learning_rate': 1.946376811594203e-05, 'epoch': 61.72}                                                              
{'loss': 0.127, 'grad_norm': 5.278438568115234, 'learning_rate': 1.9318840579710147e-05, 'epoch': 62.0}                                                                
{'loss': 0.1064, 'grad_norm': 1.8166279792785645, 'learning_rate': 1.917391304347826e-05, 'epoch': 62.29}                                                              
{'loss': 0.1131, 'grad_norm': 1.6493278741836548, 'learning_rate': 1.9028985507246378e-05, 'epoch': 62.58}                                                             
{'loss': 0.1129, 'grad_norm': 1.579367995262146, 'learning_rate': 1.8884057971014495e-05, 'epoch': 62.87}                                                              
{'loss': 0.1095, 'grad_norm': 1.2092995643615723, 'learning_rate': 1.873913043478261e-05, 'epoch': 63.14}                                                              
{'loss': 0.0991, 'grad_norm': 1.170351266860962, 'learning_rate': 1.8594202898550725e-05, 'epoch': 63.43}                                                              
{'loss': 0.1247, 'grad_norm': 2.1201727390289307, 'learning_rate': 1.844927536231884e-05, 'epoch': 63.72}                                                              
{'loss': 0.1093, 'grad_norm': 2.4026734828948975, 'learning_rate': 1.8304347826086956e-05, 'epoch': 64.0}                                                              
{'loss': 0.11, 'grad_norm': 1.2783607244491577, 'learning_rate': 1.8159420289855073e-05, 'epoch': 64.29}                                                               
{'loss': 0.1147, 'grad_norm': 1.4878393411636353, 'learning_rate': 1.8014492753623187e-05, 'epoch': 64.58}                                                             
{'loss': 0.1044, 'grad_norm': 1.2175014019012451, 'learning_rate': 1.7869565217391304e-05, 'epoch': 64.87}                                                             
{'loss': 0.1298, 'grad_norm': 1.2214471101760864, 'learning_rate': 1.772463768115942e-05, 'epoch': 65.14}                                                              
{'loss': 0.0975, 'grad_norm': 1.1780649423599243, 'learning_rate': 1.7579710144927535e-05, 'epoch': 65.43}                                                             
{'loss': 0.1151, 'grad_norm': 1.0815109014511108, 'learning_rate': 1.7434782608695652e-05, 'epoch': 65.72}                                                             
{'loss': 0.1288, 'grad_norm': 3.9661078453063965, 'learning_rate': 1.728985507246377e-05, 'epoch': 66.0}                                                               
{'loss': 0.1056, 'grad_norm': 1.3359854221343994, 'learning_rate': 1.7144927536231886e-05, 'epoch': 66.29}                                                             
{'loss': 0.1129, 'grad_norm': 1.1039479970932007, 'learning_rate': 1.7000000000000003e-05, 'epoch': 66.58}                                                             
{'loss': 0.1149, 'grad_norm': 1.5012978315353394, 'learning_rate': 1.6855072463768117e-05, 'epoch': 66.87}                                                             
{'loss': 0.1053, 'grad_norm': 1.0676566362380981, 'learning_rate': 1.6710144927536234e-05, 'epoch': 67.14}                                                             
{'loss': 0.106, 'grad_norm': 1.447808861732483, 'learning_rate': 1.656521739130435e-05, 'epoch': 67.43}                                                                
{'loss': 0.1106, 'grad_norm': 4.003887176513672, 'learning_rate': 1.6420289855072465e-05, 'epoch': 67.72}                                                              
{'loss': 0.1216, 'grad_norm': 2.2703609466552734, 'learning_rate': 1.6275362318840582e-05, 'epoch': 68.0}                                                              
{'loss': 0.1175, 'grad_norm': 1.2815005779266357, 'learning_rate': 1.6130434782608696e-05, 'epoch': 68.29}                                                             
{'loss': 0.101, 'grad_norm': 1.2107716798782349, 'learning_rate': 1.5985507246376813e-05, 'epoch': 68.58}                                                              
{'loss': 0.1215, 'grad_norm': 2.2387518882751465, 'learning_rate': 1.584057971014493e-05, 'epoch': 68.87}                                                              
{'loss': 0.1002, 'grad_norm': 1.4975793361663818, 'learning_rate': 1.5695652173913043e-05, 'epoch': 69.14}                                                             
{'loss': 0.1136, 'grad_norm': 1.4800411462783813, 'learning_rate': 1.555072463768116e-05, 'epoch': 69.43}                                                              
{'loss': 0.1107, 'grad_norm': 1.3748714923858643, 'learning_rate': 1.5405797101449278e-05, 'epoch': 69.72}                                                             
{'loss': 0.1127, 'grad_norm': 2.5629777908325195, 'learning_rate': 1.526086956521739e-05, 'epoch': 70.0}                                                               
{'loss': 0.0967, 'grad_norm': 1.1595196723937988, 'learning_rate': 1.5115942028985508e-05, 'epoch': 70.29}                                                             
{'loss': 0.1265, 'grad_norm': 1.8008384704589844, 'learning_rate': 1.4971014492753624e-05, 'epoch': 70.58}                                                             
{'loss': 0.1131, 'grad_norm': 1.6032789945602417, 'learning_rate': 1.482608695652174e-05, 'epoch': 70.87}                                                              
{'loss': 0.1083, 'grad_norm': 1.0403014421463013, 'learning_rate': 1.4681159420289856e-05, 'epoch': 71.14}                                                             
{'loss': 0.1051, 'grad_norm': 1.0945955514907837, 'learning_rate': 1.4536231884057972e-05, 'epoch': 71.43}                                                             
{'loss': 0.1053, 'grad_norm': 1.9993566274642944, 'learning_rate': 1.4391304347826087e-05, 'epoch': 71.72}                                                             
{'loss': 0.1154, 'grad_norm': 2.528916120529175, 'learning_rate': 1.4246376811594202e-05, 'epoch': 72.0}                                                               
{'loss': 0.1013, 'grad_norm': 1.7630659341812134, 'learning_rate': 1.410144927536232e-05, 'epoch': 72.29}                                                              
{'loss': 0.1159, 'grad_norm': 1.4146370887756348, 'learning_rate': 1.3956521739130435e-05, 'epoch': 72.58}                                                             
{'loss': 0.1066, 'grad_norm': 1.4315366744995117, 'learning_rate': 1.381159420289855e-05, 'epoch': 72.87}                                                              
{'loss': 0.1148, 'grad_norm': 1.3900939226150513, 'learning_rate': 1.3666666666666666e-05, 'epoch': 73.14}                                                             
{'loss': 0.1066, 'grad_norm': 1.1885932683944702, 'learning_rate': 1.3521739130434783e-05, 'epoch': 73.43}                                                             
{'loss': 0.1081, 'grad_norm': 1.311711311340332, 'learning_rate': 1.3376811594202898e-05, 'epoch': 73.72}                                                              
{'loss': 0.1088, 'grad_norm': 1.3330950736999512, 'learning_rate': 1.3231884057971014e-05, 'epoch': 74.0}                                                              
{'loss': 0.108, 'grad_norm': 1.6981829404830933, 'learning_rate': 1.308695652173913e-05, 'epoch': 74.29}                                                               
{'loss': 0.1037, 'grad_norm': 1.3349130153656006, 'learning_rate': 1.2942028985507246e-05, 'epoch': 74.58}                                                             
{'loss': 0.1109, 'grad_norm': 1.5196460485458374, 'learning_rate': 1.2797101449275361e-05, 'epoch': 74.87}                                                             
{'loss': 0.1019, 'grad_norm': 2.0437440872192383, 'learning_rate': 1.265217391304348e-05, 'epoch': 75.14}                                                              
{'loss': 0.113, 'grad_norm': 1.799347996711731, 'learning_rate': 1.2507246376811596e-05, 'epoch': 75.43}                                                               
{'loss': 0.1014, 'grad_norm': 1.216934323310852, 'learning_rate': 1.2362318840579711e-05, 'epoch': 75.72}                                                              
{'loss': 0.1033, 'grad_norm': 2.133392095565796, 'learning_rate': 1.2217391304347826e-05, 'epoch': 76.0}                                                               
{'loss': 0.1011, 'grad_norm': 1.565943956375122, 'learning_rate': 1.2072463768115942e-05, 'epoch': 76.29}                                                              
{'loss': 0.1124, 'grad_norm': 1.2929058074951172, 'learning_rate': 1.1927536231884059e-05, 'epoch': 76.58}                                                             
{'loss': 0.1127, 'grad_norm': 1.1584324836730957, 'learning_rate': 1.1782608695652174e-05, 'epoch': 76.87}                                                             
{'loss': 0.0953, 'grad_norm': 1.1686474084854126, 'learning_rate': 1.163768115942029e-05, 'epoch': 77.14}                                                              
{'loss': 0.1138, 'grad_norm': 1.7176705598831177, 'learning_rate': 1.1492753623188407e-05, 'epoch': 77.43}                                                             
{'loss': 0.1099, 'grad_norm': 1.5102365016937256, 'learning_rate': 1.1347826086956522e-05, 'epoch': 77.72}                                                             
{'loss': 0.1094, 'grad_norm': 2.6648142337799072, 'learning_rate': 1.120289855072464e-05, 'epoch': 78.0}                                                               
{'loss': 0.1052, 'grad_norm': 2.266688108444214, 'learning_rate': 1.1057971014492755e-05, 'epoch': 78.29}                                                              
{'loss': 0.1001, 'grad_norm': 1.2315551042556763, 'learning_rate': 1.091304347826087e-05, 'epoch': 78.58}                                                              
{'loss': 0.108, 'grad_norm': 1.8841272592544556, 'learning_rate': 1.0768115942028987e-05, 'epoch': 78.87}                                                              
{'loss': 0.1157, 'grad_norm': 0.9983256459236145, 'learning_rate': 1.0623188405797102e-05, 'epoch': 79.14}                                                             
{'loss': 0.1134, 'grad_norm': 1.1221427917480469, 'learning_rate': 1.0478260869565218e-05, 'epoch': 79.43}                                                             
{'loss': 0.1099, 'grad_norm': 1.603760004043579, 'learning_rate': 1.0333333333333333e-05, 'epoch': 79.72}                                                              
{'loss': 0.1019, 'grad_norm': 1.7253694534301758, 'learning_rate': 1.018840579710145e-05, 'epoch': 80.0}                                                               
{'loss': 0.101, 'grad_norm': 0.9848436713218689, 'learning_rate': 1.0043478260869566e-05, 'epoch': 80.29}                                                              
{'loss': 0.1109, 'grad_norm': 2.1352038383483887, 'learning_rate': 9.898550724637681e-06, 'epoch': 80.58}                                                              
{'loss': 0.1141, 'grad_norm': 1.4238898754119873, 'learning_rate': 9.753623188405796e-06, 'epoch': 80.87}                                                              
{'loss': 0.1025, 'grad_norm': 1.150944471359253, 'learning_rate': 9.608695652173914e-06, 'epoch': 81.14}                                                               
{'loss': 0.1117, 'grad_norm': 1.592002511024475, 'learning_rate': 9.463768115942029e-06, 'epoch': 81.43}                                                               
{'loss': 0.1028, 'grad_norm': 1.2875630855560303, 'learning_rate': 9.318840579710144e-06, 'epoch': 81.72}                                                              
{'loss': 0.103, 'grad_norm': 1.825206995010376, 'learning_rate': 9.173913043478261e-06, 'epoch': 82.0}                                                                 
{'loss': 0.1036, 'grad_norm': 1.3799102306365967, 'learning_rate': 9.028985507246379e-06, 'epoch': 82.29}                                                              
{'loss': 0.0998, 'grad_norm': 1.3648650646209717, 'learning_rate': 8.884057971014494e-06, 'epoch': 82.58}                                                              
{'loss': 0.1142, 'grad_norm': 1.1458245515823364, 'learning_rate': 8.73913043478261e-06, 'epoch': 82.87}                                                               
{'loss': 0.1171, 'grad_norm': 1.344157338142395, 'learning_rate': 8.594202898550725e-06, 'epoch': 83.14}                                                               
{'loss': 0.0945, 'grad_norm': 1.2100826501846313, 'learning_rate': 8.449275362318842e-06, 'epoch': 83.43}                                                              
{'loss': 0.1056, 'grad_norm': 1.9805448055267334, 'learning_rate': 8.304347826086957e-06, 'epoch': 83.72}                                                              
{'loss': 0.1169, 'grad_norm': 4.139400959014893, 'learning_rate': 8.159420289855073e-06, 'epoch': 84.0}                                                                
{'loss': 0.0997, 'grad_norm': 1.2778701782226562, 'learning_rate': 8.014492753623188e-06, 'epoch': 84.29}                                                              
{'loss': 0.1105, 'grad_norm': 2.420149326324463, 'learning_rate': 7.869565217391305e-06, 'epoch': 84.58}                                                               
{'loss': 0.1141, 'grad_norm': 1.6136058568954468, 'learning_rate': 7.72463768115942e-06, 'epoch': 84.87}                                                               
{'loss': 0.0992, 'grad_norm': 1.1442580223083496, 'learning_rate': 7.579710144927537e-06, 'epoch': 85.14}                                                              
{'loss': 0.1165, 'grad_norm': 1.3912739753723145, 'learning_rate': 7.434782608695652e-06, 'epoch': 85.43}                                                              
{'loss': 0.101, 'grad_norm': 1.1212671995162964, 'learning_rate': 7.289855072463768e-06, 'epoch': 85.72}                                                               
{'loss': 0.1056, 'grad_norm': 3.7850074768066406, 'learning_rate': 7.144927536231884e-06, 'epoch': 86.0}                                                               
{'loss': 0.0948, 'grad_norm': 1.562554955482483, 'learning_rate': 7.000000000000001e-06, 'epoch': 86.29}                                                               
{'loss': 0.0977, 'grad_norm': 1.4116791486740112, 'learning_rate': 6.855072463768117e-06, 'epoch': 86.58}                                                              
{'loss': 0.117, 'grad_norm': 1.5808615684509277, 'learning_rate': 6.710144927536232e-06, 'epoch': 86.87}                                                               
{'loss': 0.1157, 'grad_norm': 1.262505054473877, 'learning_rate': 6.565217391304349e-06, 'epoch': 87.14}                                                               
{'loss': 0.104, 'grad_norm': 1.8880033493041992, 'learning_rate': 6.420289855072464e-06, 'epoch': 87.43}                                                               
{'loss': 0.1064, 'grad_norm': 1.418964147567749, 'learning_rate': 6.27536231884058e-06, 'epoch': 87.72}                                                                
{'loss': 0.104, 'grad_norm': 1.7429847717285156, 'learning_rate': 6.1304347826086965e-06, 'epoch': 88.0}                                                               
{'loss': 0.0997, 'grad_norm': 1.8361164331436157, 'learning_rate': 5.985507246376812e-06, 'epoch': 88.29}                                                              
{'loss': 0.0987, 'grad_norm': 1.0510693788528442, 'learning_rate': 5.840579710144928e-06, 'epoch': 88.58}                                                              
{'loss': 0.1081, 'grad_norm': 1.235445261001587, 'learning_rate': 5.6956521739130435e-06, 'epoch': 88.87}                                                              
{'loss': 0.1148, 'grad_norm': 1.7837764024734497, 'learning_rate': 5.55072463768116e-06, 'epoch': 89.14}                                                               
{'loss': 0.1032, 'grad_norm': 1.2222647666931152, 'learning_rate': 5.405797101449275e-06, 'epoch': 89.43}                                                              
{'loss': 0.0995, 'grad_norm': 1.1602469682693481, 'learning_rate': 5.260869565217392e-06, 'epoch': 89.72}                                                              
{'loss': 0.1136, 'grad_norm': 5.209061622619629, 'learning_rate': 5.115942028985508e-06, 'epoch': 90.0}                                                                
{'loss': 0.0964, 'grad_norm': 1.3149665594100952, 'learning_rate': 4.971014492753624e-06, 'epoch': 90.29}                                                              
{'loss': 0.1035, 'grad_norm': 1.6363601684570312, 'learning_rate': 4.826086956521739e-06, 'epoch': 90.58}                                                              
{'loss': 0.1122, 'grad_norm': 2.0564887523651123, 'learning_rate': 4.6811594202898555e-06, 'epoch': 90.87}                                                             
{'loss': 0.1159, 'grad_norm': 1.6252825260162354, 'learning_rate': 4.536231884057971e-06, 'epoch': 91.14}                                                              
{'loss': 0.0981, 'grad_norm': 1.2568076848983765, 'learning_rate': 4.391304347826087e-06, 'epoch': 91.43}                                                              
{'loss': 0.106, 'grad_norm': 1.260140061378479, 'learning_rate': 4.2463768115942025e-06, 'epoch': 91.72}                                                               
{'loss': 0.1146, 'grad_norm': 3.4275829792022705, 'learning_rate': 4.1014492753623196e-06, 'epoch': 92.0}                                                              
{'loss': 0.1028, 'grad_norm': 1.4921413660049438, 'learning_rate': 3.956521739130435e-06, 'epoch': 92.29}                                                              
{'loss': 0.1024, 'grad_norm': 1.495369791984558, 'learning_rate': 3.811594202898551e-06, 'epoch': 92.58}                                                               
{'loss': 0.1038, 'grad_norm': 1.7352077960968018, 'learning_rate': 3.666666666666667e-06, 'epoch': 92.87}                                                              
{'loss': 0.0994, 'grad_norm': 1.3274511098861694, 'learning_rate': 3.521739130434783e-06, 'epoch': 93.14}                                                              
{'loss': 0.0931, 'grad_norm': 1.388846516609192, 'learning_rate': 3.3768115942028986e-06, 'epoch': 93.43}                                                              
{'loss': 0.1132, 'grad_norm': 1.6799895763397217, 'learning_rate': 3.2318840579710144e-06, 'epoch': 93.72}                                                             
{'loss': 0.1005, 'grad_norm': 1.8230457305908203, 'learning_rate': 3.0869565217391307e-06, 'epoch': 94.0}                                                              
{'loss': 0.0935, 'grad_norm': 1.3852415084838867, 'learning_rate': 2.9420289855072465e-06, 'epoch': 94.29}                                                             
{'loss': 0.1112, 'grad_norm': 1.212167739868164, 'learning_rate': 2.7971014492753623e-06, 'epoch': 94.58}                                                              
{'loss': 0.1067, 'grad_norm': 2.918677806854248, 'learning_rate': 2.6521739130434785e-06, 'epoch': 94.87}                                                              
{'loss': 0.109, 'grad_norm': 2.390415906906128, 'learning_rate': 2.5072463768115944e-06, 'epoch': 95.14}                                                               
{'loss': 0.1084, 'grad_norm': 1.324304223060608, 'learning_rate': 2.36231884057971e-06, 'epoch': 95.43}                                                                
{'loss': 0.1012, 'grad_norm': 1.1535789966583252, 'learning_rate': 2.217391304347826e-06, 'epoch': 95.72}                                                              
{'loss': 0.0978, 'grad_norm': 2.318861484527588, 'learning_rate': 2.0724637681159422e-06, 'epoch': 96.0}                                                               
{'loss': 0.1016, 'grad_norm': 1.0121952295303345, 'learning_rate': 1.927536231884058e-06, 'epoch': 96.29}                                                              
{'loss': 0.1045, 'grad_norm': 2.7865099906921387, 'learning_rate': 1.7826086956521738e-06, 'epoch': 96.58}                                                             
{'loss': 0.1047, 'grad_norm': 1.4521002769470215, 'learning_rate': 1.63768115942029e-06, 'epoch': 96.87}                                                               
{'loss': 0.0983, 'grad_norm': 1.3293863534927368, 'learning_rate': 1.4927536231884059e-06, 'epoch': 97.14}                                                             
{'loss': 0.1118, 'grad_norm': 1.9712809324264526, 'learning_rate': 1.3478260869565217e-06, 'epoch': 97.43}                                                             
{'loss': 0.1037, 'grad_norm': 1.2351139783859253, 'learning_rate': 1.2028985507246377e-06, 'epoch': 97.72}                                                             
{'loss': 0.1017, 'grad_norm': 2.3585193157196045, 'learning_rate': 1.0579710144927538e-06, 'epoch': 98.0}                                                              
{'loss': 0.1098, 'grad_norm': 1.3945415019989014, 'learning_rate': 9.130434782608697e-07, 'epoch': 98.29}                                                              
{'loss': 0.0841, 'grad_norm': 1.2592400312423706, 'learning_rate': 7.681159420289855e-07, 'epoch': 98.58}                                                              
{'loss': 0.116, 'grad_norm': 1.4198832511901855, 'learning_rate': 6.231884057971014e-07, 'epoch': 98.87}                                                               
{'loss': 0.1218, 'grad_norm': 1.991330623626709, 'learning_rate': 4.782608695652174e-07, 'epoch': 99.14}                                                               
{'loss': 0.0932, 'grad_norm': 1.1051280498504639, 'learning_rate': 3.3333333333333335e-07, 'epoch': 99.43}                                                             
{'loss': 0.1085, 'grad_norm': 1.2133610248565674, 'learning_rate': 1.8840579710144927e-07, 'epoch': 99.72}                                                             
{'loss': 0.1005, 'grad_norm': 2.074434757232666, 'learning_rate': 4.347826086956522e-08, 'epoch': 100.0}                                                               
{'train_runtime': 1805.0344, 'train_samples_per_second': 15.235, 'train_steps_per_second': 1.939, 'train_loss': 0.2052299486739295, 'epoch': 100.0}                    
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3500/3500 [30:05<00:00,  1.94it/s]